{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GeForce GTX 650\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    NOTEBOOK = 'colab'\n",
    "except:\n",
    "    import os    \n",
    "    if list(os.walk('/kaggle/input')):            \n",
    "        NOTEBOOK = 'kaggle'\n",
    "    else:\n",
    "        NOTEBOOK = 'home'\n",
    "        \n",
    "if NOTEBOOK == 'colab':\n",
    "    !pip install -q dm-sonnet\n",
    "    !pip install -q tensorflow-gan\n",
    "!grep Model: /proc/driver/nvidia/gpus/*/information | awk '{$1=\"\";print$0}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bognev/anaconda3/lib/python3.7/site-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2019 DeepMind Technologies Limited and Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Network utilities.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import functools\n",
    "import re\n",
    "import math\n",
    "# from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_gan as tfgan\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "\n",
    "def _sn_custom_getter():\n",
    "    def name_filter(name):\n",
    "        match = re.match(r'.*w(_.*)?$', name)\n",
    "        return match is not None\n",
    "\n",
    "    return tfgan.features.spectral_normalization_custom_getter(name_filter=name_filter)\n",
    "\n",
    "\n",
    "class SNGenNet(snt.Module):\n",
    "    \"\"\"As in the SN paper.\"\"\"\n",
    "    \n",
    "    def __init__(self, name='conv_gen'):\n",
    "        super(SNGenNet, self).__init__(name=name)\n",
    "        \n",
    "    @tf.function\n",
    "    def __call__(self, inputs, is_training):\n",
    "        batch_size = inputs.get_shape().as_list()[0]\n",
    "        first_shape = [4, 4, 512]\n",
    "        norm_ctor = snt.BatchNormV2\n",
    "        norm_ctor_config = {'scale': True}\n",
    "        up_tensor = snt.Linear(np.prod(first_shape))(inputs)\n",
    "        first_tensor = tf.reshape(up_tensor, shape=[batch_size] + first_shape)\n",
    "\n",
    "        net = snt.nets.ConvNet2DTranspose(\n",
    "            output_channels=[256, 128, 64, 3],\n",
    "            output_shapes=[(8, 8), (16, 16), (32, 32), (32, 32)],\n",
    "            kernel_shapes=[(4, 4), (4, 4), (4, 4), (3, 3)],\n",
    "            strides=[2, 2, 2, 1],\n",
    "            normalization_ctor=norm_ctor,\n",
    "            normalization_kwargs=norm_ctor_config,\n",
    "            normalize_final=False,\n",
    "            paddings=[snt.SAME], activate_final=False, activation=tf.nn.relu)\n",
    "        output = net(first_tensor, is_training=is_training)\n",
    "        return tf.nn.tanh(output)\n",
    "\n",
    "\n",
    "class SNMetricNet(snt.Module):\n",
    "    \"\"\"Spectral normalization discriminator (metric) architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_outputs=2, name='sn_metric'):\n",
    "        super(SNMetricNet, self).__init__(name=name)\n",
    "        self._num_outputs = num_outputs\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, inputs):\n",
    "        with tf.variable_scope('', custom_getter=_sn_custom_getter()):\n",
    "          net = snt.nets.ConvNet2D(\n",
    "              output_channels=[64, 64, 128, 128, 256, 256, 512],\n",
    "              kernel_shapes=[\n",
    "                  (3, 3), (4, 4), (3, 3), (4, 4), (3, 3), (4, 4), (3, 3)],\n",
    "              strides=[1, 2, 1, 2, 1, 2, 1],\n",
    "              paddings=[snt.SAME], activate_final=True,\n",
    "              activation=functools.partial(tf.nn.leaky_relu, alpha=0.1))\n",
    "          linear = snt.Linear(self._num_outputs)\n",
    "        output = linear(snt.BatchFlatten()(net(inputs)))\n",
    "        return output\n",
    "\n",
    "\n",
    "class MLPGeneratorNet(snt.Module):\n",
    "    \"\"\"MNIST generator net.\"\"\"\n",
    "\n",
    "    def __init__(self, name='mlp_generator'):\n",
    "        super(MLPGeneratorNet, self).__init__(name=name)\n",
    "        self.net = snt.nets.MLP([500, 500, 784], activation=tf.nn.leaky_relu)\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, inputs, is_training=True):\n",
    "        del is_training\n",
    "        out = self.net(inputs)\n",
    "        out = tf.nn.tanh(out)\n",
    "        out = snt.Reshape([28, 28, 1])(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLPMetricNet(snt.Module):\n",
    "    \"\"\"Same as in Grover and Ermon, ICLR workshop 2017.\"\"\"\n",
    "\n",
    "    def __init__(self, num_outputs=2, name='mlp_metric'):\n",
    "        super(MLPMetricNet, self).__init__(name=name)\n",
    "        self._layer_size = [500, 500, num_outputs]\n",
    "        self.net = snt.nets.MLP(self._layer_size,\n",
    "                       activation=tf.nn.leaky_relu)\n",
    "        \n",
    "    @tf.function\n",
    "    def __call__(self, inputs):\n",
    "        output = self.net(snt.flatten(inputs))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3\n",
    "\n",
    "# Copyright 2019 DeepMind Technologies Limited and Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Tools for latent optimisation.\"\"\"\n",
    "\n",
    "\n",
    "# import nets\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class ModelOutputs(\n",
    "    collections.namedtuple('AdversarialModelOutputs',\n",
    "                           ['optimization_components', 'debug_ops'])):\n",
    "  \"\"\"All the information produced by the adversarial module.\n",
    "\n",
    "  Fields:\n",
    "\n",
    "    * `optimization_components`: A dictionary. Each entry in this dictionary\n",
    "      corresponds to a module to train using their own optimizer. The keys are\n",
    "      names of the components, and the values are `common.OptimizationComponent`\n",
    "      instances. The keys of this dict can be made keys of the configuration\n",
    "      used by the main train loop, to define the configuration of the\n",
    "      optimization details for each module.\n",
    "    * `debug_ops`: A dictionary, from string to a scalar `tf.Tensor`. Quantities\n",
    "      used for tracking training.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "class OptimizationComponent(\n",
    "    collections.namedtuple('OptimizationComponent', ['loss', 'vars'])):\n",
    "  \"\"\"Information needed by the optimizer to train modules.\n",
    "\n",
    "  Usage:\n",
    "      `optimizer.minimize(\n",
    "          opt_compoment.loss, var_list=opt_component.vars)`\n",
    "\n",
    "  Fields:\n",
    "\n",
    "    * `loss`: A `tf.Tensor` the loss of the module.\n",
    "    * `vars`: A list of variables, the ones which will be used to minimize the\n",
    "      loss.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, expected):\n",
    "  \"\"\"The cross entropy classification loss between logits and expected values.\n",
    "\n",
    "  The loss proposed by the original GAN paper: https://arxiv.org/abs/1406.2661.\n",
    "\n",
    "  Args:\n",
    "    logits: a `tf.Tensor`, the model produced logits.\n",
    "    expected: a `tf.Tensor`, the expected output.\n",
    "\n",
    "  Returns:\n",
    "    A scalar `tf.Tensor`, the average loss obtained on the given inputs.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the logits do not have shape [batch_size, 2].\n",
    "  \"\"\"\n",
    "\n",
    "  num_logits = logits.get_shape()[1]\n",
    "  if num_logits != 2:\n",
    "    raise ValueError(('Invalid number of logits for cross_entropy_loss! '\n",
    "                      'cross_entropy_loss supports only 2 output logits!'))\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=expected))\n",
    "\n",
    "\n",
    "def optimise_and_sample(init_z, module, data, is_training):\n",
    "  \"\"\"Optimising generator latent variables and sample.\"\"\"\n",
    "\n",
    "  if module.num_z_iters == 0:\n",
    "    z_final = init_z\n",
    "  else:\n",
    "    init_z = _project_z(init_z, module.z_project_method)    \n",
    "    z = tf.Variable(init_z)\n",
    "    for i in range(module.num_z_iters):\n",
    "      with tf.GradientTape() as tape:\n",
    "        loop_samples = module.generator(z, is_training)\n",
    "        gen_loss = module.gen_loss_fn(data, loop_samples)\n",
    "      z_grad = tape.gradient(gen_loss, z)\n",
    "      z.assign_sub(module.z_step_size * z_grad)\n",
    "      z.assign(_project_z(z, module.z_project_method))\n",
    "#       z = tf.Variable()\n",
    "    z_final = z\n",
    "  return module.generator(z_final, is_training), z_final\n",
    "\n",
    "\n",
    "def get_optimisation_cost(initial_z, optimised_z):\n",
    "  optimisation_cost = tf.reduce_mean(\n",
    "      tf.reduce_sum((optimised_z - initial_z)**2, -1))\n",
    "  return optimisation_cost\n",
    "\n",
    "\n",
    "def _project_z(z, project_method='clip'):\n",
    "  \"\"\"To be used for projected gradient descent over z.\"\"\"\n",
    "  if project_method == 'norm':\n",
    "    z_p = tf.nn.l2_normalize(z, axis=-1)\n",
    "  elif project_method == 'clip':\n",
    "    z_p = tf.clip_by_value(z, -1, 1)\n",
    "  else:\n",
    "    raise ValueError('Unknown project_method: {}'.format(project_method))\n",
    "  return z_p\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "\n",
    "  def preprocess(self, x):\n",
    "    return x * 2 - 1\n",
    "\n",
    "  def postprocess(self, x):\n",
    "    return (x + 1) / 2.\n",
    "\n",
    "\n",
    "def _get_np_data(data_processor, dataset, split='train'):\n",
    "  \"\"\"Get the dataset as numpy arrays.\"\"\"\n",
    "  index = 0 if split == 'train' else 1\n",
    "  if dataset == 'mnist':\n",
    "    # Construct the dataset.\n",
    "    x, _ = tf.keras.datasets.mnist.load_data()[index]\n",
    "    # Note: tf dataset is binary so we convert it to float.\n",
    "    x = x.astype(np.float32)\n",
    "    x = x / 255.\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "\n",
    "  if dataset == 'cifar':\n",
    "    x, _ = tf.keras.datasets.cifar10.load_data()[index]\n",
    "    x = x.astype(np.float32)\n",
    "    x = x / 255.\n",
    "\n",
    "  if data_processor:\n",
    "    # Normalize data if a processor is given.\n",
    "    x = data_processor.preprocess(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def make_output_dir(output_dir):\n",
    "  tf.print('Creating output dir %s', output_dir)\n",
    "  if not tf.io.gfile.isdir(output_dir):\n",
    "    tf.io.gfile.makedirs(output_dir)\n",
    "\n",
    "\n",
    "def get_ckpt_dir(output_dir):\n",
    "  ckpt_dir = os.path.join(output_dir, 'ckpt')\n",
    "  if not tf.io.gfile.isdir(ckpt_dir):\n",
    "    tf.io.gfile.makedirs(ckpt_dir)\n",
    "  return ckpt_dir\n",
    "\n",
    "\n",
    "def get_real_data_for_eval(num_eval_samples, dataset, split='valid'):\n",
    "  data = _get_np_data(data_processor=None, dataset=dataset, split=split)\n",
    "  data = data[:num_eval_samples]\n",
    "  return tf.constant(data)\n",
    "\n",
    "\n",
    "def get_summaries(ops, logger):\n",
    "  summaries = []\n",
    "  for name, op in ops.items():\n",
    "    # Ensure to log the value ops before writing them in the summary.\n",
    "    # We do this instead of a hook to ensure IS/FID are never computed twice.\n",
    "    print_op = tf.print(name, [op], output_stream=logger)#tf.logging.info)\n",
    "    with tf.control_dependencies([print_op]):\n",
    "      summary = tf.summary.scalar(name, op)\n",
    "      summaries.append(summary)\n",
    "  return summaries\n",
    "\n",
    "\n",
    "def get_train_dataset(data_processor, dataset, batch_size):\n",
    "  \"\"\"Creates the training data tensors.\"\"\"\n",
    "  x_train = _get_np_data(data_processor, dataset, split='train')\n",
    "  tf.print(x_train.shape)\n",
    "  # Create the TF dataset.\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "\n",
    "  # Shuffle and repeat the dataset for training.\n",
    "  # This is required because we want to do multiple passes through the entire\n",
    "  # dataset when training.\n",
    "  dataset = dataset.shuffle(60000).batch(batch_size)\n",
    "\n",
    "  # Batch the data and return the data batch.\n",
    "  # one_shot_iterator = dataset.batch(batch_size)#.make_one_shot_iterator()\n",
    "  # data_batch = one_shot_iterator.__iter__()\n",
    "  # return next(data_batch)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_generator(dataset):\n",
    "  if dataset == 'mnist':\n",
    "    return MLPGeneratorNet()\n",
    "  if dataset == 'cifar':\n",
    "    return SNGenNet()\n",
    "\n",
    "\n",
    "def get_metric_net(dataset, num_outputs=2):\n",
    "  if dataset == 'mnist':\n",
    "    return MLPMetricNet(num_outputs)\n",
    "  if dataset == 'cifar':\n",
    "    return SNMetricNet(num_outputs)\n",
    "\n",
    "\n",
    "def make_prior(num_latents):\n",
    "  # Zero mean, unit variance prior.\n",
    "  prior_mean = tf.zeros(shape=(num_latents), dtype=tf.float32)\n",
    "  prior_scale = tf.ones(shape=(num_latents), dtype=tf.float32)\n",
    "\n",
    "  return tfd.Normal(loc=prior_mean, scale=prior_scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Limited and Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"GAN modules.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# class TrainableVariable(tf.Module):\n",
    "#   def __call__(self, x):\n",
    "#     if not hasattr(self, 'w'):\n",
    "#       self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))\n",
    "#     return tf.matmul(x, self.w)\n",
    "\n",
    "class TrainableVariable(snt.Module):\n",
    "    \"\"\"Provides learnable parameter Tensor.\"\"\"\n",
    "    def __init__(self, shape, dtype=tf.float32, initializers=None,  name=\"trainable_variable\"):\n",
    "        super(TrainableVariable, self).__init__(name=name)\n",
    "        self._shape = tuple(shape)\n",
    "        self._dtype = dtype\n",
    "        self._initializers = initializers\n",
    "\n",
    "#     @tf.function\n",
    "    def __call__(self):\n",
    "        \"\"\"Connects the TrainableTensor module into the graph.\n",
    "        Returns:\n",
    "          A Tensor of shape as determined in the constructor.\n",
    "        \"\"\"\n",
    "        self._w = tf.Variable(name = \"z_step_size\",\n",
    "                              shape=self._shape,\n",
    "                              dtype=self._dtype,\n",
    "                              initial_value=self._initializers,\n",
    "                              )\n",
    "        return self._w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CS(snt.Module):\n",
    "    \"\"\"Compressed Sensing Module.\"\"\"\n",
    "\n",
    "    def __init__(self, metric_net, generator,\n",
    "               num_z_iters, z_step_size, z_project_method, optimizer):\n",
    "        \"\"\"Constructs the module.\n",
    "\n",
    "        Args:\n",
    "          metric_net: the measurement network.\n",
    "          generator: The generator network. A sonnet module. For examples, see\n",
    "            `nets.py`.\n",
    "          num_z_iters: an integer, the number of latent optimisation steps.\n",
    "          z_step_size: an integer, latent optimisation step size.\n",
    "          z_project_method: the method for projecting latent after optimisation,\n",
    "            a string from {'norm', 'clip'}.\n",
    "        \"\"\"\n",
    "        super(CS, self).__init__()\n",
    "        self._measure = metric_net\n",
    "        self.generator = generator\n",
    "        self.num_z_iters = num_z_iters\n",
    "        self.z_project_method = z_project_method\n",
    "        self._log_step_size_module = TrainableVariable(shape=[], dtype=tf.float32, initializers=math.log(z_step_size))\n",
    "        self.z_step_size = tf.exp(self._log_step_size_module())\n",
    "        self.optimizer = optimizer\n",
    "        self.z = None\n",
    "\n",
    "\n",
    "\n",
    "#     @tf.function\n",
    "    def step(self, data, generator_inputs):\n",
    "        \"\"\"Connects the components and returns the losses, outputs and debug ops.\n",
    "\n",
    "        Args:\n",
    "          data: a `tf.Tensor`: `[batch_size, ...]`. There are no constraints on the\n",
    "            rank\n",
    "            of this tensor, but it has to be compatible with the shapes expected\n",
    "            by the discriminator.\n",
    "          generator_inputs: a `tf.Tensor`: `[g_in_batch_size, ...]`. It does not\n",
    "            have to have the same batch size as the `data` tensor. There are not\n",
    "            constraints on the rank of this tensor, but it has to be compatible\n",
    "            with the shapes the generator network supports as inputs.\n",
    "\n",
    "        Returns:\n",
    "          An `ModelOutputs` instance.\n",
    "        \"\"\"\n",
    "        debug_ops = {}\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            samples, optimised_z = optimise_and_sample(\n",
    "                generator_inputs, self, data, is_training=True)\n",
    "            optimisation_cost = get_optimisation_cost(generator_inputs,\n",
    "                                                        optimised_z)\n",
    "            initial_samples = self.generator(generator_inputs, is_training=True)\n",
    "            generator_loss = tf.reduce_mean(self.gen_loss_fn(data, samples))\n",
    "            # compute the RIP loss\n",
    "            # (\\sqrt{F(x_1 - x_2)^2} - \\sqrt{(x_1 - x_2)^2})^2\n",
    "            # as a triplet loss for 3 pairs of images.\n",
    "            r1 = self._get_rip_loss(samples, initial_samples)\n",
    "            r2 = self._get_rip_loss(samples, data)\n",
    "            r3 = self._get_rip_loss(initial_samples, data)\n",
    "            rip_loss = tf.reduce_mean((r1 + r2 + r3) / 3.0)\n",
    "            total_loss = generator_loss + rip_loss\n",
    "        optimization_components = self._build_optimization_components(\n",
    "            generator_loss=total_loss)\n",
    "        grads = tape.gradient(optimization_components.loss, optimization_components.vars)\n",
    "        self.optimizer.apply(grads, optimization_components.vars)\n",
    "        \n",
    "        \n",
    "\n",
    "        debug_ops['rip_loss'] = rip_loss\n",
    "        debug_ops['recons_loss'] = tf.reduce_mean(\n",
    "          tf.norm(snt.flatten(samples)\n",
    "                  - snt.flatten(data), axis=-1))\n",
    "        debug_ops['z_step_size'] = self.z_step_size\n",
    "        debug_ops['opt_cost'] = optimisation_cost\n",
    "        debug_ops['gen_loss'] = generator_loss\n",
    "\n",
    "        return ModelOutputs(\n",
    "            optimization_components, debug_ops)\n",
    "\n",
    "    def _get_rip_loss(self, img1, img2):\n",
    "        r\"\"\"Compute the RIP loss from two images.\n",
    "\n",
    "          The RIP loss: (\\sqrt{F(x_1 - x_2)^2} - \\sqrt{(x_1 - x_2)^2})^2\n",
    "\n",
    "        Args:\n",
    "          img1: an image (x_1), 4D tensor of shape [batch_size, W, H, C].\n",
    "          img2: an other image (x_2), 4D tensor of shape [batch_size, W, H, C].\n",
    "        \"\"\"\n",
    "\n",
    "        m1 = self._measure(img1)\n",
    "        m2 = self._measure(img2)\n",
    "\n",
    "        img_diff_norm = tf.norm(snt.flatten(img1)\n",
    "                                - snt.flatten(img2), axis=-1)\n",
    "        m_diff_norm = tf.norm(m1 - m2, axis=-1)\n",
    "\n",
    "        return tf.square(img_diff_norm - m_diff_norm)\n",
    "\n",
    "    def _get_measurement_error(self, target_img, sample_img):\n",
    "        \"\"\"Compute the measurement error of sample images given the targets.\"\"\"\n",
    "\n",
    "        m_targets = self._measure(target_img)\n",
    "        m_samples = self._measure(sample_img)\n",
    "\n",
    "        return tf.reduce_sum(tf.square(m_targets - m_samples), -1)\n",
    "\n",
    "    def gen_loss_fn(self, data, samples):\n",
    "        \"\"\"Generator loss as latent optimisation's error function.\"\"\"\n",
    "        return self._get_measurement_error(data, samples)\n",
    "\n",
    "    def _build_optimization_components(\n",
    "        self, generator_loss=None, discriminator_loss=None):\n",
    "        \"\"\"Create the optimization components for this module.\"\"\"\n",
    "\n",
    "        metric_vars = _get_and_check_variables(self._measure)\n",
    "        generator_vars = _get_and_check_variables(self.generator)\n",
    "        step_vars = self._log_step_size_module.trainable_variables\n",
    "\n",
    "\n",
    "        assert discriminator_loss is None\n",
    "        optimization_components = OptimizationComponent(\n",
    "            generator_loss, generator_vars + metric_vars + step_vars)\n",
    "        return optimization_components\n",
    "\n",
    "\n",
    "def _get_and_check_variables(module):\n",
    "    # module_variables = module.get_all_variables()\n",
    "    module_variables = module.net.trainable_variables\n",
    "    if not module_variables:\n",
    "        raise ValueError(\n",
    "            'Module {} has no variables! Variables needed for training.'.format(\n",
    "                module.module_name))\n",
    "\n",
    "    # TensorFlow optimizers require lists to be passed in.\n",
    "    return module_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op SummaryWriter in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op CreateSummaryFileWriter in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-19d715d31189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reload_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--logdir logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(args_string)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0mparsed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m   \u001b[0mstart_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorboard/manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(arguments, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m   \u001b[0mend_time_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time_seconds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend_time_seconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_interval_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0msubprocess_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/\n",
    "log_dir = \"logs/\" + dt.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS.__flags\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for name in list(flags.FLAGS):\n",
    "        delattr(flags.FLAGS, name)\n",
    "\n",
    "FIRST = False\n",
    "\n",
    "def run(main=None, argv=None):\n",
    "  args = argv[1:] if argv else None\n",
    "  flags_passthrough = app.parse_flags_with_usage(args=args)\n",
    "  main = main or sys.modules['__main__'].main\n",
    "  main(sys.argv[:1] + flags_passthrough)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Limited and Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Training script.\"\"\"\n",
    "\n",
    "if FIRST:\n",
    "    del_all_flags(FLAGS)\n",
    "FIRST = True\n",
    "\n",
    "\n",
    "# import logging as log\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_boolean('debug', True, 'Produces debugging output.')\n",
    "flags.DEFINE_string(\n",
    "    'mode', 'recons', 'Model mode.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_training_iterations', 20,\n",
    "    'Number of training iterations.')\n",
    "flags.DEFINE_integer(\n",
    "    'batch_size', 64, 'Training batch size.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_measurements', 25, 'The number of measurements')\n",
    "flags.DEFINE_integer(\n",
    "    'num_latents', 100, 'The number of latents')\n",
    "flags.DEFINE_integer(\n",
    "    'num_z_iters', 3, 'The number of latent optimisation steps.')\n",
    "flags.DEFINE_float(\n",
    "    'z_step_size', 0.01, 'Step size for latent optimisation.')\n",
    "flags.DEFINE_string(\n",
    "    'z_project_method', 'norm', 'The method to project z.')\n",
    "flags.DEFINE_integer(\n",
    "    'summary_every_step', 1000,\n",
    "    'The interval at which to log debug ops.')\n",
    "flags.DEFINE_integer(\n",
    "    'export_every', 1000,\n",
    "    'The interval at which to export samples.')\n",
    "flags.DEFINE_string(\n",
    "    'dataset', 'mnist', 'The dataset used for learning (cifar|mnist.')\n",
    "flags.DEFINE_float('learning_rate', 1e-4, 'Learning rate.')\n",
    "flags.DEFINE_string(\n",
    "    'output_dir', './cs', 'Location where to save output files.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    if FLAGS.debug:\n",
    "        tf.print('non-flag arguments:', argv)\n",
    "    tf.print(FLAGS.output_dir)\n",
    "\n",
    "    tf.print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "    tf.print(\"Tensorflow eager execution: {}\".format(tf.executing_eagerly()))\n",
    "    tf.print(\"    Sonnet version: {}\".format(snt.__version__))\n",
    "    tf.print(\"    Numpy  version: {}\".format(np.__version__))\n",
    "\n",
    "    output_dir = os.path.join(FLAGS.output_dir, 'summaries')\n",
    "    os.system('rm -rf ' + output_dir)\n",
    "\n",
    "    make_output_dir(FLAGS.output_dir)\n",
    "    data_processor = DataProcessor()\n",
    "    \n",
    "    tf.print('Learning rate: %d', FLAGS.learning_rate)\n",
    "\n",
    "    # Construct optimizers.\n",
    "    optimizer = snt.optimizers.Adam(FLAGS.learning_rate)\n",
    "\n",
    "    # Create the networks and models.\n",
    "    generator = get_generator(FLAGS.dataset)\n",
    "    metric_net = get_metric_net(FLAGS.dataset, FLAGS.num_measurements)\n",
    "    model = CS(metric_net, generator, FLAGS.num_z_iters, FLAGS.z_step_size, FLAGS.z_project_method, optimizer)\n",
    "\n",
    "    sample_exporter = FileExporter(os.path.join(FLAGS.output_dir, 'reconstructions'))\n",
    "    \n",
    "    t = tqdm(range(FLAGS.num_training_iterations * int(60000/64)), unit='sig', unit_scale=FLAGS.batch_size, position=0)\n",
    "    prior = make_prior(FLAGS.num_latents)\n",
    "    tf.print('starting training')        \n",
    "    for num_epochs in range(FLAGS.num_training_iterations):        \n",
    "        images = get_train_dataset(data_processor, FLAGS.dataset, FLAGS.batch_size)        \n",
    "        for i, batch in enumerate(images):            \n",
    "            generator_inputs = prior.sample(FLAGS.batch_size)\n",
    "            model_output = model.step(batch, generator_inputs)\n",
    "            if num_epochs == 0 and i==0:\n",
    "                print(snt.format_variables(model_output.optimization_components.vars))\n",
    "            debug_ops = model_output.debug_ops\n",
    "            reconstructions, _ = optimise_and_sample(\n",
    "                generator_inputs, model, batch, is_training=False)\n",
    "            t.update(1)               \n",
    "            debug_ops['it'] = i\n",
    "            if i % FLAGS.summary_every_step == 0:\n",
    "                t.write('Epoch = {}/{} (lr_mult = {:0.09f}, loss = {:0.02f}, accuracy = {:0.02f}, incorrect = {} done.'.format(\n",
    "                num_epochs, FLAGS.num_training_iterations, FLAGS.learning_rate, model_output.optimization_components.loss, 0, 0))  \n",
    "                with writer.as_default():\n",
    "                    for name, op in debug_ops.items():\n",
    "                        tf.summary.scalar(name, op, debug_ops['it'])\n",
    "\n",
    "    return\n",
    "run(main, ['Avoid', 'problems', 'with', 'FLGAS', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
